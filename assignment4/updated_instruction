For this exercise, you will be using Pig to run MapReduce jobs in AWS to process about 0.5 TB of text. The details and problem statements are provided in assignment4.md with errata given at the end of this post. To minimise the time of reserving AWS machines, I advise that you prepare the Pig scripts for problems 1 to 4 beforehand and call them problem1.pig, problem2.pig, etc. Check the errata when preparing the scripts.

I will be assuming familiarity with Linux and that you already have an AWS account. For Windows users, please install Git which has a Linux terminal emulator known as Git Bash. Contrary to the instructions given in GitHub, if you are using Git Bash, doing this assignment on Windows is as easy as doing it on Linux or Mac. (In fact, I completed the whole assignment on my Android tablet!)

Here are the steps to get started.

Complete the opt-in quiz and contact AWS Support to request for credits.
Log in to AWS Management Console.
To minimise network traffic, at the top of the console (next to your name), change the location to Oregon since all the data are stored there.
Create an SSH key pair and download the key (call it pig.pem) to your project folder.
Next, create an EMR cluster. To save on storage, you can disable logging. Choose a current generation instance type like m4.large which has one of the lowest rates available. Use 1 instance (1 master and 0 core node) at the beginning. (Actually, one is enough for problems 0 to 3.) Remember to select the key pair created in the previous step. Leave everything else as default.
Wait… The public DNS should appear after 5 min. Take note of the IP address given by the front part of the DNS: ec2-aa-bb-cc-dd, where aa, bb, cc, dd are 4 numbers for the IP address of the master node.
Add an inbound rule to the security group, ElasticMapReduce-master, to open port 22 for SSH login from anywhere.
Open Git Bash (for Windows) or your favourite terminal. Run the following commands to copy the Pig scripts to the master node (cd to your project folder first) and to log in (replace aa.bb.cc.dd below by the actual IP address of the master node):

At this point, Pig may not be ready yet. (You can try typing pig at the command line to see if it works.) The cluster can take up to 20 min for everything to set up properly, whose status is reflected on the website automatically but not instantaneously. To run Pig interactively, simply type pig at the command line. The command prompt will be changed to grunt> for entering Pig commands. You can then copy and paste the lines from example.pig one by one to execute the script interactively. When you are done, press CTRL-D to exit from Grunt.

If you have prepared the scripts beforehand, you can continue to use the same master node for problems 1 to 3 as each script should take at most 10 min to complete using only one instance. To run a script in the background, type at the command line


The command nohup will allow Pig to continue running in the background when you are disconnected from the master node.

When your script has finished running, open another Git Bash or terminal to copy the log file from the master node to your local machine.


[See Appendix below for an extract of my log file for problem 1.]

From the log file, you should be able to answer the first 3 questions of problem 1. Note that this is not the file to be submitted for grading. The uploaded file, as with other parts of the assignment, should contain only one number to answer the question asked of each part.

For problem 2, after the script has finished, you can merge and order your results into one file with this command


[See Appendix below for an extract of my results for problem 2.]

Submit your answer for problem 2. Get it right before going to problem 3. Again, your submission for each part of the problem is a text file containing only one number.

Important: Before attempting problem 4, make sure you have answered problem 2 correctly.

While the script for problem 3 is running, start up another EMR cluster with 20 instances (1 master, 19 core) for problem 4 following the steps above. 20 is the maximum you can have unless you make a special request. You can use the same SSH key. You will be running over the full data set using the script for problem 2 with minor changes to the input data set, output results location, and perhaps the degree of parallelism. The script should take less than an hour to complete, so check your job every 20 min or so by doing


[See Appendix below for an extract of my results for problem 4.]

Warning: Be sure to go back to the EMR console to TERMINATE your EMR clusters when you are done with Pig. If not, AWS will continue charging you for reserving the instances.

Notes

It is not necessary to set up port forwarding or a proxy to monitor your jobs. All required information to answer the questions can be found in your log files.
As of 2017/03/26, the price of launching one m4.large instance in EMR is 15¢ per hour, so a rough estimate of the total cost is 4 USD.
Don't forget to copy the log files and results back to your local machine before terminating your EMR clusters. Do not copy the results for problems 1 and 3 as the files are too big and not needed.
The command hadoop has changed to hdfs in the new version.
Feel free to leave a comment below. However, if you encounter a problem, it is better to create a new thread for your issue.

Good luck!

Errata

For problem 3, the subject should match '.*rdfabout\\.com.*' (2 slashes instead of 1) when running over chunk-000.
For problem 4, the name of the data set is 's3n://uw-cse-344-oregon.aws.amazon.com/btc-2010-chunk-*' (no backslash at the end).
